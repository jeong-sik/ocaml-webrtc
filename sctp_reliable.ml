(** RFC 4960 SCTP Reliable Transport Layer

    Implements the full SCTP state machine:
    - SACK (Selective Acknowledgment) handling
    - Congestion Control (Slow Start, Congestion Avoidance)
    - Retransmission (T3-rtx timer, Fast Retransmit)

    This layer sits on top of sctp.ml encoding and sctp_transport.ml network I/O
    to provide reliable, congestion-controlled data transfer.

    @author Second Brain
    @since ocaml-webrtc 0.3.0
*)

(** {1 SACK Types - RFC 4960 Section 3.3.4} *)

type gap_block = {
  start_offset: int;  (** Offset from cumulative TSN *)
  end_offset: int;
}

type dup_tsn = int32

type sack = {
  cumulative_tsn_ack: int32;  (** All TSNs <= this have been received *)
  a_rwnd: int;                (** Advertised receiver window *)
  gap_blocks: gap_block list; (** Gaps in received TSNs *)
  dup_tsns: dup_tsn list;     (** Duplicate TSNs received *)
}

(** {1 Retransmission Queue Entry} *)

(* NOTE: rtx_entry is now handled internally by Sctp_ring_buffer *)
(* The ring buffer stores InFlight entries with equivalent fields:
   - chunk, sent_at, retransmit_count, miss_indications, fast_retransmit
   - acked state is represented by the Acked variant *)

(** {1 Receive Buffer for SACK Generation} *)

(** Optimized gap range for O(k) SACK generation.
    Stores absolute TSN ranges instead of individual TSNs.
    k = number of gaps, typically 0-5 in practice *)
type gap_range = {
  gr_start: int32;  (** First TSN in this gap range *)
  gr_end: int32;    (** Last TSN in this gap range *)
}

type recv_buffer = {
  mutable cumulative_tsn: int32;  (** Highest in-order TSN received *)
  mutable gap_ranges: gap_range list;  (** Sorted list of out-of-order TSN ranges *)
  mutable a_rwnd: int;
  mutable dup_tsns: int32 list;  (** Duplicate TSNs received since last SACK (RFC 4960 ยง6.2) *)
}

(** {1 Congestion Control State - RFC 4960 Section 7.2} *)

type congestion_state = {
  mutable cwnd: int;              (** Congestion window in bytes *)
  mutable ssthresh: int;          (** Slow start threshold *)
  mutable partial_bytes_acked: int;
  mutable in_fast_recovery: bool;
  mutable fast_recovery_exit_point: int32;
}

(** {1 RTO Calculator - RFC 6298} *)

type rto_state = {
  mutable srtt: float;    (** Smoothed RTT *)
  mutable rttvar: float;  (** RTT variance *)
  mutable rto: float;     (** Current RTO in seconds *)
  rto_min: float;
  rto_max: float;
}

(** {1 Reliable Transport State} *)

type stats = {
  mutable data_chunks_sent: int;
  mutable data_chunks_recv: int;
  mutable sacks_sent: int;
  mutable sacks_recv: int;
  mutable retransmissions: int;
  mutable fast_retransmissions: int;
  mutable timeouts: int;
  mutable bytes_sent: int;
  mutable bytes_acked: int;
}

type t = {
  config: Sctp.config;

  (* Sender state - Ring buffer manages TSN assignment and flight tracking *)
  rtx_queue: Sctp_ring_buffer.t;  (** Ring buffer for retransmission queue *)

  (* Receiver state *)
  recv_buf: recv_buffer;

  (* Congestion control *)
  cc: congestion_state;

  (* RTO *)
  rto: rto_state;

  (* RACK - time-based loss detection (RFC 8985 adaptation) *)
  rack: Sctp_rack.t;

  (* Timer state *)
  mutable t3_rtx_start: float option;  (** T3-rtx timer start time *)

  stats: stats;
}

(** {1 Constants - RFC 4960} *)

let mtu_overhead = 28  (* IP + UDP headers *)
let sctp_header_size = 12
let data_chunk_header_size = 16
let sack_chunk_header_size = 16

(** Max DATA chunks payload per MTU *)
let max_payload_size config =
  config.Sctp.mtu - mtu_overhead - sctp_header_size - data_chunk_header_size

(** {1 Creation} *)

let create_stats () = {
  data_chunks_sent = 0;
  data_chunks_recv = 0;
  sacks_sent = 0;
  sacks_recv = 0;
  retransmissions = 0;
  fast_retransmissions = 0;
  timeouts = 0;
  bytes_sent = 0;
  bytes_acked = 0;
}

(** Generate random initial TSN per RFC 4960 Section 5.3.1
    "The initial TSN SHOULD be set to a random value" *)
let random_initial_tsn () =
  (* Use Int32 to avoid sign issues, range 1 to 2^31-1 *)
  let r = Random.int32 0x7FFFFFFFl in
  if r = 0l then 1l else r

let create ?(config = Sctp.default_config) ?initial_tsn () =
  let initial_cwnd = min (4 * config.mtu) (max (2 * config.mtu) 4380) in
  let initial_tsn = match initial_tsn with
    | Some tsn -> tsn  (* Allow override for testing/handshake *)
    | None -> random_initial_tsn ()
  in
  {
    config;
    (* Ring buffer manages TSN assignment and flight_size internally *)
    rtx_queue = Sctp_ring_buffer.create ~capacity:4096 ~initial_tsn ();

    (* Receiver's cumulative_tsn must be (initial_tsn - 1) so that
       the expected next TSN (cumulative + 1) matches sender's first TSN.
       In real SCTP, this is negotiated during the 4-way handshake. *)
    recv_buf = {
      cumulative_tsn = Int32.pred initial_tsn;  (* 999 -> expects 1000 *)
      gap_ranges = [];  (* O(k) gap tracking - no Hashtbl! *)
      a_rwnd = config.a_rwnd;
      dup_tsns = [];  (* Duplicate TSNs for SACK reporting *)
    };

    cc = {
      cwnd = initial_cwnd;
      ssthresh = config.a_rwnd;  (* Initial ssthresh = rwnd *)
      partial_bytes_acked = 0;
      in_fast_recovery = false;
      fast_recovery_exit_point = 0l;
    };

    rto = {
      srtt = 0.0;
      rttvar = 0.0;
      rto = float_of_int config.rto_initial_ms /. 1000.0;
      rto_min = float_of_int config.rto_min_ms /. 1000.0;
      rto_max = float_of_int config.rto_max_ms /. 1000.0;
    };

    (* RACK - time-based loss detection for faster retransmit *)
    rack = Sctp_rack.create ();

    t3_rtx_start = None;
    stats = create_stats ();
  }

(** {1 SACK Encoding/Decoding} *)

(** Encode SACK chunk - RFC 4960 Section 3.3.4 *)
let encode_sack sack =
  let num_gap_blocks = List.length sack.gap_blocks in
  let num_dup_tsns = List.length sack.dup_tsns in
  let chunk_len = sack_chunk_header_size + (num_gap_blocks * 4) + (num_dup_tsns * 4) in

  let buf = Bytes.create chunk_len in
  (* Chunk header *)
  Bytes.set buf 0 (Char.chr 3);  (* SACK type *)
  Bytes.set buf 1 (Char.chr 0);  (* Flags *)
  Bytes.set_int16_be buf 2 chunk_len;

  (* SACK specific *)
  Bytes.set_int32_be buf 4 sack.cumulative_tsn_ack;
  Bytes.set_int32_be buf 8 (Int32.of_int sack.a_rwnd);
  Bytes.set_int16_be buf 12 num_gap_blocks;
  Bytes.set_int16_be buf 14 num_dup_tsns;

  (* Gap blocks *)
  let offset = ref 16 in
  List.iter (fun gap ->
    Bytes.set_int16_be buf !offset gap.start_offset;
    Bytes.set_int16_be buf (!offset + 2) gap.end_offset;
    offset := !offset + 4
  ) sack.gap_blocks;

  (* Duplicate TSNs *)
  List.iter (fun tsn ->
    Bytes.set_int32_be buf !offset tsn;
    offset := !offset + 4
  ) sack.dup_tsns;

  buf

(** Decode SACK chunk *)
let decode_sack buf =
  if Bytes.length buf < sack_chunk_header_size then
    Error "SACK too short"
  else
    let chunk_type = Bytes.get buf 0 |> Char.code in
    if chunk_type <> 3 then
      Error (Printf.sprintf "Not a SACK chunk: type=%d" chunk_type)
    else begin
      let cumulative_tsn_ack = Bytes.get_int32_be buf 4 in
      let a_rwnd = Bytes.get_int32_be buf 8 |> Int32.to_int in
      let num_gap_blocks = Bytes.get_int16_be buf 12 in
      let num_dup_tsns = Bytes.get_int16_be buf 14 in

      (* Parse gap blocks *)
      let gap_blocks = ref [] in
      for i = 0 to num_gap_blocks - 1 do
        let off = 16 + (i * 4) in
        let start_offset = Bytes.get_int16_be buf off in
        let end_offset = Bytes.get_int16_be buf (off + 2) in
        gap_blocks := { start_offset; end_offset } :: !gap_blocks
      done;

      (* Parse duplicate TSNs *)
      let dup_tsns = ref [] in
      let dup_start = 16 + (num_gap_blocks * 4) in
      for i = 0 to num_dup_tsns - 1 do
        let off = dup_start + (i * 4) in
        let tsn = Bytes.get_int32_be buf off in
        dup_tsns := tsn :: !dup_tsns
      done;

      Ok {
        cumulative_tsn_ack;
        a_rwnd;
        gap_blocks = List.rev !gap_blocks;
        dup_tsns = List.rev !dup_tsns;
      }
    end

(** {1 Receiver: Process Incoming DATA, Generate SACK} *)

(** TSN comparison (handles wraparound) *)
let tsn_lt a b =
  let diff = Int32.sub a b in
  diff < 0l

let tsn_le a b =
  a = b || tsn_lt a b

let tsn_gt a b = tsn_lt b a

(** Record received TSN and update cumulative TSN - O(k) algorithm
    where k = number of gap ranges (typically 0-5)
    Returns: true if this is a NEW TSN, false if duplicate/already received *)
let record_received t tsn =
  let recv = t.recv_buf in

  (* Helper to record duplicate - RFC 4960 ยง6.2: report up to 4 duplicates *)
  let record_dup () =
    if List.length recv.dup_tsns < 4 && not (List.mem tsn recv.dup_tsns) then
      recv.dup_tsns <- tsn :: recv.dup_tsns
  in

  (* Only process TSNs > cumulative - if not, it's a duplicate *)
  if not (tsn_gt tsn recv.cumulative_tsn) then begin
    record_dup ();
    false  (* Duplicate - already received *)
  end
  else begin
    let expected = Int32.succ recv.cumulative_tsn in
    if tsn = expected then begin
      (* In order - advance cumulative *)
      recv.cumulative_tsn <- tsn;
      (* Merge any gap ranges that are now contiguous with cumulative *)
      let rec merge_gaps cum = function
        | [] -> (cum, [])
        | ({ gr_start; gr_end } as range) :: rest ->
          if gr_start = Int32.succ cum then
            (* This gap range is now contiguous - absorb it *)
            merge_gaps gr_end rest
          else
            (* Gap still exists *)
            (cum, range :: rest)
      in
      let (new_cum, new_gaps) = merge_gaps tsn recv.gap_ranges in
      recv.cumulative_tsn <- new_cum;
      recv.gap_ranges <- new_gaps;
      true  (* New TSN received in-order *)
    end else begin
      (* Out of order - insert into gap_ranges maintaining sorted order *)
      (* Returns (new_ranges, is_new) where is_new indicates if TSN was new *)
      let rec insert_tsn = function
        | [] ->
          (* No existing ranges, create new one *)
          ([{ gr_start = tsn; gr_end = tsn }], true)
        | ({ gr_start; gr_end } as range) :: rest ->
          if tsn_lt tsn gr_start then begin
            (* TSN comes before this range *)
            if Int32.succ tsn = gr_start then
              (* Extend range backward *)
              ({ gr_start = tsn; gr_end } :: rest, true)
            else
              (* New isolated range before *)
              ({ gr_start = tsn; gr_end = tsn } :: range :: rest, true)
          end else if tsn_le tsn gr_end then
            (* TSN already in this range (duplicate) *)
            (range :: rest, false)
          else if tsn = Int32.succ gr_end then begin
            (* Extend range forward - check if we can merge with next *)
            match rest with
            | { gr_start = next_start; gr_end = next_end } :: rest2
              when Int32.succ tsn = next_start ->
              (* Merge current and next range *)
              ({ gr_start; gr_end = next_end } :: rest2, true)
            | _ ->
              (* Just extend current range *)
              ({ gr_start; gr_end = tsn } :: rest, true)
          end else begin
            (* TSN comes after this range *)
            let (new_rest, is_new) = insert_tsn rest in
            (range :: new_rest, is_new)
          end
      in
      let (new_ranges, is_new) = insert_tsn recv.gap_ranges in
      recv.gap_ranges <- new_ranges;
      if not is_new then record_dup ();
      is_new
    end
  end

(** Generate SACK from receive state - O(k) where k = number of gap ranges
    Gap ranges are pre-computed in record_received, just convert to offsets
    RFC 4960 ยง6.2: Include duplicate TSNs received since last SACK *)
let generate_sack t =
  let recv = t.recv_buf in

  (* Convert absolute TSN ranges to relative offsets from cumulative *)
  let gap_blocks =
    List.map (fun { gr_start; gr_end } ->
      {
        start_offset = Int32.to_int (Int32.sub gr_start recv.cumulative_tsn);
        end_offset = Int32.to_int (Int32.sub gr_end recv.cumulative_tsn);
      }
    ) recv.gap_ranges
  in

  (* Capture duplicate TSNs and clear for next SACK *)
  let dup_tsns = List.rev recv.dup_tsns in  (* Oldest first *)
  recv.dup_tsns <- [];

  {
    cumulative_tsn_ack = recv.cumulative_tsn;
    a_rwnd = recv.a_rwnd;
    gap_blocks;
    dup_tsns;
  }

(** {1 Sender: Process SACK, Update cwnd} *)

(** Update RTO based on RTT measurement - RFC 6298 *)
let update_rto t rtt =
  let rto = t.rto in
  if rto.srtt = 0.0 then begin
    (* First measurement *)
    rto.srtt <- rtt;
    rto.rttvar <- rtt /. 2.0;
  end else begin
    (* Subsequent measurements *)
    let alpha = 0.125 in
    let beta = 0.25 in
    rto.rttvar <- (1.0 -. beta) *. rto.rttvar +. beta *. abs_float (rto.srtt -. rtt);
    rto.srtt <- (1.0 -. alpha) *. rto.srtt +. alpha *. rtt;
  end;
  rto.rto <- max rto.rto_min (min rto.rto_max (rto.srtt +. 4.0 *. rto.rttvar))

(** Process SACK and update congestion state - RFC 4960 Section 7
    Optimized with Ring Buffer for O(1) operations *)
let process_sack t sack =
  t.stats.sacks_recv <- t.stats.sacks_recv + 1;

  let now = Unix.gettimeofday () in
  let cum_tsn = sack.cumulative_tsn_ack in
  let mtu = t.config.mtu in

  (* Process cumulative TSN ack - Ring Buffer handles marking and returns bytes+RTT *)
  let (cum_bytes, rtt_sample) =
    Sctp_ring_buffer.process_cumulative_ack t.rtx_queue cum_tsn now
  in

  (* Update RTO if we got an RTT sample from first-transmission chunk *)
  (match rtt_sample with
  | Some rtt -> update_rto t rtt
  | None -> ());

  (* Check gap blocks for selectively acked TSNs *)
  let gap_bytes = ref 0 in
  List.iter (fun gap ->
    for offset = gap.start_offset to gap.end_offset do
      let tsn = Int32.add cum_tsn (Int32.of_int offset) in
      (* Only ack if not already acked *)
      if not (Sctp_ring_buffer.is_acked t.rtx_queue tsn) then begin
        let bytes_freed = Sctp_ring_buffer.ack t.rtx_queue tsn in
        gap_bytes := !gap_bytes + bytes_freed
      end
    done
  ) sack.gap_blocks;

  (* Total bytes acked (cumulative + gap blocks) *)
  let newly_acked = cum_bytes + !gap_bytes in
  t.stats.bytes_acked <- t.stats.bytes_acked + newly_acked;

  (* Advance head to reclaim ring buffer space *)
  ignore (Sctp_ring_buffer.advance_head t.rtx_queue);

  (* RACK-based loss detection (RFC 8985 adaptation)
     Time-based detection is 71% faster than "3 duplicate SACKs" method.
     Process SACK through RACK to update RTT and detect losses. *)
  let gap_block_list = List.map (fun g ->
    (g.start_offset, g.end_offset)
  ) sack.gap_blocks in

  let (_rack_rtt, rack_lost_tsns) =
    Sctp_rack.process_sack t.rack ~cumulative_tsn:cum_tsn ~gap_blocks:gap_block_list
  in

  (* Mark RACK-detected losses for fast retransmit *)
  List.iter (fun lost_tsn ->
    if Sctp_ring_buffer.mark_retransmit t.rtx_queue lost_tsn then
      t.stats.fast_retransmissions <- t.stats.fast_retransmissions + 1
  ) rack_lost_tsns;

  (* Also keep traditional 3-miss indication as fallback for gap blocks
     RFC 4960 Section 7.2.4: Miss indication ONLY applies when SACK reports
     gap blocks (selective acknowledgment) *)
  if sack.gap_blocks <> [] then
    Sctp_ring_buffer.iter_unacked_above t.rtx_queue cum_tsn
      (fun _tsn miss_count ->
        (* Fast retransmit after 3 miss indications - RFC 4960 Section 7.2.4 *)
        if miss_count >= 3 then begin
          t.stats.fast_retransmissions <- t.stats.fast_retransmissions + 1;
          true  (* Mark for fast retransmit *)
        end else
          false);

  (* Congestion control update - RFC 4960 Section 7.2.1/7.2.2 *)
  let cc = t.cc in
  if newly_acked > 0 then begin
    if cc.cwnd <= cc.ssthresh then begin
      (* Slow Start: cwnd += min(newly_acked, mtu) *)
      cc.cwnd <- cc.cwnd + min newly_acked mtu
    end else begin
      (* Congestion Avoidance: cwnd += mtu * mtu / cwnd *)
      cc.partial_bytes_acked <- cc.partial_bytes_acked + newly_acked;
      if cc.partial_bytes_acked >= cc.cwnd then begin
        cc.partial_bytes_acked <- cc.partial_bytes_acked - cc.cwnd;
        cc.cwnd <- cc.cwnd + mtu
      end
    end
  end;

  (* Cancel T3-rtx timer if all data acked *)
  if Sctp_ring_buffer.is_empty t.rtx_queue then
    t.t3_rtx_start <- None

(** {1 Retransmission Timer} *)

(** Check if T3-rtx timer expired and handle timeout *)
let check_t3_rtx_timeout t =
  match t.t3_rtx_start with
  | None -> []
  | Some start_time ->
    let now = Unix.gettimeofday () in
    if now -. start_time >= t.rto.rto then begin
      (* Timeout! *)
      t.stats.timeouts <- t.stats.timeouts + 1;

      (* RFC 4960 Section 7.2.3: Adjust ssthresh and cwnd *)
      let cc = t.cc in
      cc.ssthresh <- max (cc.cwnd / 2) (4 * t.config.mtu);
      cc.cwnd <- t.config.mtu;

      (* Double RTO (exponential backoff) *)
      t.rto.rto <- min t.rto.rto_max (t.rto.rto *. 2.0);

      (* Mark all in-flight chunks for retransmission *)
      let to_retransmit = Sctp_ring_buffer.mark_all_for_retransmit t.rtx_queue in
      t.stats.retransmissions <- t.stats.retransmissions + List.length to_retransmit;

      (* Reset timer *)
      t.t3_rtx_start <- Some now;

      to_retransmit
    end else
      []

(** Get chunks marked for fast retransmit *)
let get_fast_retransmit_chunks t =
  Sctp_ring_buffer.get_and_clear_fast_retransmit t.rtx_queue

(** {1 Sender: Queue Data for Transmission} *)

(** Check if we can send more data (cwnd allows) *)
let can_send t =
  Sctp_ring_buffer.flight_size t.rtx_queue < t.cc.cwnd

(** Queue data chunk for transmission.
    Ring buffer handles flight_size tracking internally. *)
let queue_data t chunk =
  (* Ring buffer stores the chunk and tracks flight_size *)
  let success = Sctp_ring_buffer.enqueue_with_tsn t.rtx_queue chunk in
  if success then begin
    t.stats.data_chunks_sent <- t.stats.data_chunks_sent + 1;
    t.stats.bytes_sent <- t.stats.bytes_sent + Bytes.length chunk.Sctp.user_data;

    (* RACK: Record send timestamp for time-based loss detection *)
    Sctp_rack.record_send t.rack chunk.Sctp.tsn;

    (* Start T3-rtx timer if not running *)
    if t.t3_rtx_start = None then
      t.t3_rtx_start <- Some (Unix.gettimeofday ())
  end

(** Allocate next TSN - delegates to ring buffer *)
let alloc_tsn t =
  Sctp_ring_buffer.alloc_tsn t.rtx_queue

(** {1 Statistics and Debug} *)

let get_stats t = t.stats

let get_cwnd t = t.cc.cwnd
let get_ssthresh t = t.cc.ssthresh
let get_flight_size t = Sctp_ring_buffer.flight_size t.rtx_queue
let get_rto t = t.rto.rto

(** Check if all queued data has been acknowledged *)
let all_acked t = Sctp_ring_buffer.is_empty t.rtx_queue

let pp_stats fmt s =
  Format.fprintf fmt
    "sent=%d recv=%d sacks=%d/%d rtx=%d fast_rtx=%d timeouts=%d bytes=%d/%d"
    s.data_chunks_sent s.data_chunks_recv
    s.sacks_sent s.sacks_recv
    s.retransmissions s.fast_retransmissions s.timeouts
    s.bytes_sent s.bytes_acked

let pp_cc_state fmt t =
  Format.fprintf fmt "cwnd=%d ssthresh=%d flight=%d rto=%.3fs"
    t.cc.cwnd t.cc.ssthresh (Sctp_ring_buffer.flight_size t.rtx_queue) t.rto.rto

(** Debug: Get receiver's cumulative TSN *)
let get_cumulative_tsn t = t.recv_buf.cumulative_tsn

(** Debug: Get number of gap ranges in receiver *)
let get_gap_count t = List.length t.recv_buf.gap_ranges

(** Debug: Get gap ranges as (start, end) pairs - for tracing out-of-order delivery *)
let get_gap_ranges t =
  List.map (fun { gr_start; gr_end } ->
    (Int32.to_int gr_start, Int32.to_int gr_end)
  ) t.recv_buf.gap_ranges

(** Debug: Get next TSN that would be assigned by sender *)
let get_next_tsn t = Sctp_ring_buffer.next_tsn t.rtx_queue

(** Debug: Get last assigned TSN from sender side *)
let get_last_sent_tsn t = Int32.pred (get_next_tsn t)
